---
authors: [pratikdas]
title: "Getting Started with AWS Kinesis"
categories: ["aws"]
date: 2022-01-20T00:00:00
excerpt: "Amazon Kinesis is a family of managed services for collecting and processing streaming data in real-time. Stream processing platforms are an integral part of the big data ecosystem. Examples of streaming data are data collected from website click-streams, marketing, and financial information, social media feeds, IoT sensors, and monitoring and operational logs. In this article, we will introduce Amazon Kinesis and understand its core concepts of creating data streams, sending, and receiving data from streams, and deriving analytical insights using the different services of the family: Kinesis Data Stream, Kinesis Data Firehose, Kinesis Data Analytics, and Kinesis Video Streams."
image: images/stock/0115-2021-1200x628-branded.jpg
url: getting-started-with-aws-kinesis
---

Amazon Kinesis is a family of managed services for collecting and processing streaming data in real-time. Stream processing platforms are an integral part of the big data ecosystem. Examples of streaming data are data collected from website click-streams, marketing, and financial information, social media feeds, IoT sensors, and monitoring and operational logs. 

Amazon Kinesis includes the following services each focussing on different stages of handling  streaming data :
1. Kinesis Data Stream for ingestion and storage of streaming data
2. Kinesis Firehose for delivery of streaming data
3. Kinesis Analytics for running analysis programs over the ingested data for deriving analytical insights
4. Kinesis Video Streams for ingestion and storage of video data

In this article, we will introduce these services and understand their core concepts of creating data streams, sending, and receiving data from streams, and deriving analytical insights by working through some examples.

{{% github "https://github.com/thombergs/code-examples/tree/master/aws/kinesis" %}}

## What is Streaming Data

Streaming data is a pattern of data being generated continuously (in a stream) by multiple data sources which typically send the data records simultaneously. Due to its continuous nature, streaming data is also called unbounded data as opposed to bounded data processed by batch processing systems.

Streaming data includes a wide variety of data such as log files generated by customers using mobile or web applications, e-commerce purchases, in-game player activity, information from social networks, financial trading floors, or geospatial services, and telemetry from connected devices or instrumentation in data centers.

Streaming data is processed sequentially and incrementally either by one record at a time or in batches of records aggregated over sliding time windows. 

## What is Amazon Kinesis
Amazon Kinesis is a fully managed streaming data platform for processing streaming data. It provides four specialized services based on the type of stream data processing we want to perform to suit our use case:

- **Kinesis Data Streams**: The Kinesis Data Streams service is used to capture streaming data produced by various data sources in real-time. Producer applications write to the Kinesis Data Stream and consumer applications connected to the stream read the data for different types of processing.

- **Kinesis Data Firehose**: With Kinesis Data Firehose, we do not need to write applications or manage resources. We configure data producers to send data to Kinesis Data Firehose, and it automatically delivers the data to the specified destination. We can also configure Kinesis Data Firehose to transform the data before delivering it.

- **Kinesis Data Analytics**:With Amazon Kinesis Data Analytics we can process and analyze streaming data. It provides an efficient and scalable environment to run Flink applications.

- **Kinesis Video Streams**: Amazon Kinesis Video Streams is a fully managed AWS service that we can use to stream live media from video or audio capturing devices to the AWS Cloud, or build applications for real-time video processing or batch-oriented video analytics.

Let us understand these services in the next sections. In each section, we will first introduce the key concepts of the service and then work through some examples.

## Ingesting Streaming Data with Kinesis Data Streams

The Kinesis Data Streams service is used to collect and process streaming data as soon as it is produced (in real-time). The ingested data is emitted to another data store after some transformation or used to run any processing task for producing real-time metrics and analytics. The data is stored for 24 hours by default which can be configured to store the data beyond 24 hours up to 365 days.

A common use of Kinesis Data Streams is the real-time aggregation of data followed by loading the aggregated data into a data warehouse or map-reduce cluster.

### Data Stream, Shards, and Records
When using Kinesis Data Streams, we first set up a data stream and then build producer applications that write data to the data stream and consumer applications that read and process the data from the data stream in real-time:

{{% image alt="Create Kinesis Delivery Stream" src="images/posts/aws-kinesis/kds-shards.png" %}}

The data stream is composed of multiple data carriers called shards, as we can see in this diagram. Each shard provides a fixed unit of capacity. The data capacity of a data stream is a function of the number of shards in the stream. The total capacity of the data stream is the sum of the capacities of all the shards it is composed of.

The data stored in the shard is called a record.

Each shard contains a sequence of data records. Each data record has a sequence number that is assigned by Kinesis Data Streams.

### Creating a Kinesis Data Stream
Let us first create our data stream where we can send our data. 

We can create a Kinesis data stream either by using the AWS Kinesis Data Streams Management Console, from the AWS CLI or using the `CreateStream` operation of AWS SDK. Our code for creating a data stream with AWS SDK for Java looks like this: 

```java
public class DataStreamResourceHelper {

	public static void createDataStream() {
		 KinesisClient kinesisClient = getKinesisClient();
		 
		 CreateStreamRequest createStreamRequest 
		    = CreateStreamRequest
			 .builder()
			 .streamName(Constants.MY_DATA_STREAM)
			 .streamModeDetails(
			 	StreamModeDetails
			 	.builder()
			 	.streamMode(StreamMode.ON_DEMAND)
			 	.build())
			 .build();

		 CreateStreamResponse createStreamResponse 
			   = kinesisClient.createStream(createStreamRequest);
				...
				...
		 
	}
	
	private static KinesisClient getKinesisClient() {
		AwsCredentialsProvider credentialsProvider = 
		        ProfileCredentialsProvider.create(Constants.AWS_PROFILE_NAME);
		
		KinesisClient kinesisClient = KinesisClient
				.builder()
				.credentialsProvider(credentialsProvider)
				.region(Region.US_EAST_1).build();
		return kinesisClient;
	}
}

```
In the code snippet, we are creating a data stream with `on-demand` capacity mode. The capacity mode of `On-Demand` is used for unpredictable workloads which scale the capacity of the data stream automatically in response to varying data traffic

With the data stream created, we will look at how to add data to this stream in the next section.

### Data Ingestion - Writing Data to Kinesis Data Streams
Applications that write data to Kinesis Data Streams are called `producers`. Producer applications can be custom built in a supported programming language using AWS SDK or by using the Kinesis Producer Library (KPL). 

We can also use Kinesis Agent which is a stand-alone application that we can run as an agent on Linux-based server environments such as web servers, log servers, and database servers. 

Before adding data, it is also important to understand the structure of data that is added to a Kinesis Data Stream.
**Data is written to a Kinesis Data Stream as a record.**

A record in a Kinesis data stream consists of multiple identifiers. **A record is composed of a sequence number, partition key, and data blob.** The maximum size of a data blob (the data payload before Base64-encoding) is 1 megabyte (MB).

**A sequence number is a unique identifier for each record.** Sequence number is assigned by Amazon Kinesis Data Streams when a producer application calls the `PutRecord()` or `PutRecords()` operation to add data to a Kinesis Data Stream.

**Partition key is used to segregate and route records to different shards of a stream.** We need to specify the partition key in the producer application while adding data to a Kinesis stream. 

For example, if we have a stream with two shards: `shard1` and `shard2`, we can write our producer application to use two partition keys: `key1` and `key2` so that all records with `key1` are added to `shard1` and all records with `key2` are added to `shard2`.

Let us create a producer application in Java that will use the AWS SDK's `PutRecord()` operation for adding a single record and the `PutRecords()` operation for adding multiple records to the Kinesis Data Stream.

We have created this producer application as a Maven project and added a Maven dependency in our `pom.xml` as shown below:

```xml
   <dependencies>
  		<dependency>
		    <groupId>software.amazon.awssdk</groupId>
		    <artifactId>kinesis</artifactId>
		</dependency>
   </dependencies>
   <dependencyManagement>
    <dependencies>
      <dependency>
        <groupId>software.amazon.awssdk</groupId>
        <artifactId>bom</artifactId>
        <version>2.17.116</version>
        <type>pom</type>
        <scope>import</scope>
      </dependency>
    </dependencies>
   </dependencyManagement>
```
In the `pom.xml`, we have added the `kinesis` library as a dependency after adding `bom` for AWS SDK.

Here is the code for adding a single event to the Kinesis Data Stream that we created in the previous step:

```java
public class EventSender {

    private static final Logger logger = Logger
        .getLogger(EventSender.class.getName());

	public static void main(String[] args) {
		sendEvent();
	}
	
	public static void sendEvent() {
        KinesisClient kinesisClient = getKinesisClient();
        
        // Set the partition key
		String partitionKey = "partitionKey1";

		// Assign a sequence number
		String sequenceNumberForOrdering = "1";

		// Create the data to be sent to Kinesis Data Stream in bytes
		SdkBytes data 
		= SdkBytes.fromByteBuffer(
			ByteBuffer.wrap("Test data".getBytes()));

        // Create the request for putRecord method
		PutRecordRequest putRecordRequest 
		   = PutRecordRequest
		   .builder()
		   .streamName(Constants.MY_DATA_STREAM)
		   .partitionKey(partitionKey)
		   .sequenceNumberForOrdering(sequenceNumberForOrdering)
		   .data(data)
		   .build();
		
		 // Call the method to write the record to Kinesis Data Stream
		 PutRecordResponse putRecordsResult 
		 = kinesisClient.putRecord(putRecordRequest);
        
		 logger.info("Put Result" + putRecordsResult);
         kinesisClient.close();
	}

	private static KinesisClient getKinesisClient() {
		AwsCredentialsProvider credentialsProvider = 
		       ProfileCredentialsProvider
		       .create(Constants.AWS_PROFILE_NAME);
		
		KinesisClient kinesisClient = KinesisClient
				.builder()
				.credentialsProvider(credentialsProvider)
				.region(Region.US_EAST_1).build();
		return kinesisClient;
	}
}
```
Here we are first creating the request object for the `putRecord()` method with the name of the Kinesis Data Stream, partition key, the sequence number, and the data to be sent in bytes. Then we have invoked the `putRecord()` method on the `kinesisClient` to add a record to the stream.


Running this program gives the following output:

```shell
INFO: Put ResultPutRecordResponse(ShardId=shardId-000000000001, SequenceNumber=49626569155656830268862440193769593466823195675894743058)
```
We can see the `shardId` identifier of the shard where the record is added along with the sequence number of the record.

Let us next add multiple events to the Kinesis data stream by using the `putRecords()` method as shown below:

```java
public class EventSender {

    private static final Logger logger = 
             Logger.getLogger(EventSender.class.getName());

	public static void main(String[] args) {
		sendEvents();

	}
	
	public static void sendEvents() {
        KinesisClient kinesisClient = getKinesisClient();
        
		String partitionKey = "partitionKey1";
        
        List <PutRecordsRequestEntry> putRecordsRequestEntryList = new ArrayList<>(); 

        // Create 5 records for adding to the Kinesis Data Stream
        for (int i = 0; i < 5; i++) {
        	SdkBytes data = SdkBytes
        			.fromByteBuffer(ByteBuffer.wrap(("Test data "+i).getBytes()));
        	
            PutRecordsRequestEntry putRecordsRequestEntry  
                    = PutRecordsRequestEntry.builder()         
	                    .data(data)
	                    .partitionKey(partitionKey) // Same partition key for all records
	                    .build();
            
            putRecordsRequestEntryList.add(putRecordsRequestEntry); 
        }
        
        // Create the request for putRecords method
        PutRecordsRequest putRecordsRequest 
						        = PutRecordsRequest
						        .builder()
						        .streamName(Constants.MY_DATA_STREAM)
						        .records(putRecordsRequestEntryList)
						        .build();
        
		PutRecordsResponse putRecordsResult = kinesisClient
				.putRecords(putRecordsRequest);
		
        logger.info("Put records Result" + putRecordsResult);
        kinesisClient.close();
	}

	private static KinesisClient getKinesisClient() {
      ...
      ...
	}
}

```

Here we are first creating the request object for the `putRecords()` method with the name of the data stream, partition key, and sequence number. Running this program gives the following output:

```shell
...ResultPutRecordsResponse(FailedRecordCount=0, 
	Records=[
	PutRecordsResultEntry(SequenceNumber=49626569155656830268862440193770802392642928158972051474, ShardId=shardId-000000000001), 

	PutRecordsResultEntry(SequenceNumber=49626569155656830268862440193772011318462542788146757650, ShardId=shardId-000000000001), 

	PutRecordsResultEntry(SequenceNumber=49626569155656830268862440193773220244282157417321463826, ShardId=shardId-000000000001), 

	PutRecordsResultEntry(SequenceNumber=49626569155656830268862440193774429170101772046496170002, ShardId=shardId-000000000001), 

	PutRecordsResultEntry(SequenceNumber=49626569155656830268862440193775638095921386675670876178, ShardId=shardId-000000000001)])
```
In this output, we can see the same `shardId` for all the records added to the stream. This is because we are using the same partition key for all our records, they have been put into the same shard with shardId : `shardId-000000000001`.

As mentioned before, other than the AWS SDK, we can use the Kinesis Producer Library (KPL) or the Kinesis agent for adding data to a Kinesis Data Stream:

1. **Kinesis Producer Library (KPL)**: KPL is a library written in C++ for adding data into a Kinesis data stream. It runs as a child process to the main user process. So in case the child process stops due to any when connecting or writing to a Kinesis Data Stream, the main process continues to run.

2. **Amazon Kinesis Agent**: Kinesis Agent is a stand-alone application that we can run as an agent on Linux-based server environments such as web servers, log servers, and database servers. The agent continuously monitors a set of files and collects and sends new data to Kinesis Data Streams. Please refer to the official documentation for guidance on configuring Kinesis Agent in a Linux-based server environment.

### Data Consumption - Reading Data from Kinesis Data Streams
We can develop two types of consumers for Kinesis Data Streams: shared fan-out consumers which share the throughput of the stream and enhanced fan-out consumers with dedicated throughput. To learn about the differences between them, and to see how you can create each type of consumer, see Reading Data from Amazon Kinesis Data Streams.

With the data ingested in our data stream, let us get into creating a consumer application that can process all the data from a Kinesis data stream.

We can create consumer applications using:
1. AWS Lambda
2. Kinesis Client Library
3. Kinesis Data Firehose
4. Kinesis Data Analytics
5. AWS SDK

Let us use the AWS Java SDK to read records written to the Kinesis Data Stream created earlier. We had added data to this stream in the previous section using the `putRecords()` operation:

```java
public class EventConsumer {
	
	public static void receiveEvents() {
        KinesisClient kinesisClient = getKinesisClient();
        
		String shardId = "shardId-000000000001";
        
		GetShardIteratorRequest getShardIteratorRequest 
		  = GetShardIteratorRequest
			  .builder()
			  .streamName(Constants.MY_DATA_STREAM)
			  .shardId(shardId)
			  .shardIteratorType(ShardIteratorType.TRIM_HORIZON.name())
			  .build();

		GetShardIteratorResponse getShardIteratorResponse 
		        = kinesisClient
		          .getShardIterator(getShardIteratorRequest );

		String shardIterator = getShardIteratorResponse.shardIterator();
		logger.info("shardIterator " + shardIterator);
		
		while(shardIterator != null) {
			GetRecordsRequest getRecordsRequest 
			  = GetRecordsRequest
				  .builder()
				  .shardIterator(shardIterator)
				  .limit(5)
				  .build();

			GetRecordsResponse getRecordsResponse 
			    = kinesisClient.getRecords(getRecordsRequest );
	        		
			List<Record> records = getRecordsResponse.records();
			
			logger.info("count "+records.size());
			records.forEach(record->{
				byte[] dataInBytes = record.data().asByteArray();
				logger.info(new String(dataInBytes));
			});
			
			shardIterator = getRecordsResponse.nextShardIterator();
		}

		kinesisClient.close();
	}
	
	private static KinesisClient getKinesisClient() {
		AwsCredentialsProvider credentialsProvider = 
		          ProfileCredentialsProvider.create(Constants.AWS_PROFILE_NAME);
		
		KinesisClient kinesisClient = KinesisClient
				.builder()
				.credentialsProvider(credentialsProvider)
				.region(Region.US_EAST_1).build();

		return kinesisClient;
	}

}

```

Here in the `receiveEvents()` method, we are invoking the `getRecords()` method to read the ingested records from the Kinesis Data Stream. Before this we are getting hold of a `shardIterator` to iterate through all the shards in the stream which is used to create the request object for the `getRecords()` method.

However, in most practical situations, an AWS Lambda function is more convenient for reading records from a Kinesis Data Stream. A Lambda function with Kinesis Data Stream as an event source is automatically configured with a poller for the stream which reads and passes the record from the stream to the Lambda function as a parameter. 

With our data ingested in the Kinesis Data Stream, we will look at delivering and processing this ingested data using the Kinesis Data Firehose and Kinesis Data Analytics services in the subsequent sections.

## Delivering Streaming Data with Kinesis Data Firehose

Kinesis Data Firehose is a fully managed service that is used to deliver streaming data to a destination. 

One or more data producers send their streaming data into some kind of "pipe" called a delivery stream which optionally applies some transformation to the data before delivering it to a destination.

The incoming streaming data is buffered in the delivery stream till it reaches a particular size or exceeds a certain time interval before it is delivered to the destination.

An example of a data producer is a web server that sends log data to a delivery stream. 

We can also configure the delivery stream to read streaming data from a Kinesis Data Stream and deliver it to a destination. 

### Creating a Kinesis Firehose Delivery Stream

Let us take a closer look at the Kinesis Data Firehose service by creating a Firehose delivery stream. We can create a Firehose delivery stream using the AWS console, AWS SDK, or infrastructure as a service like CloudFormation.

For our example, let us use the AWS console for creating the delivery stream as shown below:

{{% image alt="Create Kinesis Delivery Stream" src="images/posts/aws-kinesis/kdf_create.png" %}}

We configure a delivery stream in Firehose with a source and a destination. 

The source of a Kinesis Data Firehose delivery stream can be :
1. Kinesis Data Stream 
2. `Direct PUT` which means an application producer can send data to the delivery stream using a direct `PUT` operation.

Here we have chosen the source as `Direct PUT`.

Similarly, the delivery stream can send data to the following destinations:

1. Amazon Simple Storage Service (Amazon S3), 
2. Amazon Redshift
3. Amazon OpenSearch Service
4. Splunk, and
5. Any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers like Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, and Sumo Logic. 

We have chosen our destination as `S3` and configured an S3 bucket that will receive the streaming data delivered by our Firehose delivery stream.

Apart from this we also need to assign an IAM service role to the Kinesis Firehose service with an access policy that allows it to write to the S3 bucket.

The delivery stream which we created using this configuration looks like this:

{{% image alt="Created Kinesis Delivery Stream" src="images/posts/aws-kinesis/kdf_created.png" %}}

In this screenshot, we can observe a few more properties of the delivery stream like `Data transformation` and `Dynamic partitioning` which we will understand in the subsequent sections. We can also see the status of the delivery stream as `Active` which means it can receive streaming data. The initial status of the delivery stream is CREATING. 

We are now ready to send streaming data to our Firehose delivery stream which will deliver this data to the configured destination.

### Sending Data to a Kinesis Firehose Delivery Stream

As explained earlier, a Firehose delivery stream can receive data from two kinds of sources: a Kinesis delivery stream or from an application producing data. We have already seen how to put data to a Kinesis data stream. If we connect this data stream to a Kinesis firehose delivery stream, data will be automatically sent to the destination.

We created our firehose delivery stream in the previous section with the `Direct PUT` option due to which we can send data directly to the Firehose delivery stream from a producer application. 

A very simplified code snippet for sending a single record to a Kinesis Firehose data stream looks like this:

```java
public class FirehoseEventSender {
	private final static Logger logger = 
	     Logger.getLogger(FirehoseEventSender.class.getName());

	public static void main(String[] args) {
		new FirehoseEventSender().sendEvent();
	}
	
	public void sendEvent() {
		String deliveryStreamName= "PUT-S3-5ZGgA";
		
		String data = "Test data" + "\n";
		
		Record record = Record
							.builder()
							.data(SdkBytes
								   .fromByteArray(data.getBytes()))
							.build();
		
		PutRecordRequest putRecordRequest = 
							       PutRecordRequest
									.builder()
									.deliveryStreamName(deliveryStreamName)
									.record(record)
									.build();

		FirehoseClient firehoseClient = getFirehoseClient();

		// Put record into the DeliveryStream
		PutRecordResponse putRecordResponse = 
		                  firehoseClient.putRecord(putRecordRequest);
		
		logger.info("record ID:: " + putRecordResponse.recordId());

		firehoseClient.close();
	}
	
	private static FirehoseClient getFirehoseClient() {
		AwsCredentialsProvider credentialsProvider = 
		              ProfileCredentialsProvider
		              .create(Constants.AWS_PROFILE_NAME);
		
		FirehoseClient kinesisClient = FirehoseClient
				.builder()
				.credentialsProvider(credentialsProvider)
				.region(Constants.AWS_REGION).build();
		return kinesisClient;
	}
	

}

```

Here we are calling the `putRecord()` method for adding a single record to the delivery stream. 

The Kinesis Data Firehose API offers two operations for sending data to the Firehose delivery stream: PutRecord and PutRecordBatch. PutRecord() sends one data record within one call and PutRecordBatch() can send multiple data records within one call.

Other than this, we can send streaming data to a Delivery Stream using Firehose Agent

### Data Transformation

We can configure the Kinesis Data Firehose delivery stream to transform incoming source data and deliver the transformed data to destinations by attaching a lambda function.

Kinesis Data Firehose can invoke a Lambda function to transform incoming source data and deliver the transformed data to destinations. We can enable Kinesis Data Firehose data transformation when we create our delivery stream.

### Dynamic Partitioning
Dynamic partitioning enables you to continuously partition streaming data in Kinesis Data Firehose by using keys within data (for example, customer_id or transaction_id) and then deliver the data grouped by these keys into corresponding Amazon Simple Storage Service (Amazon S3) prefixes. 

### Data Delivery
After our delivery stream receives the streaming data, it is automatically delivered to the configured destination. Each destination type supported by Firehose has specific configurations for data delivery.

For data delivery to S3, Kinesis Data Firehose concatenates multiple incoming records based on the buffering configuration of our delivery stream. It then delivers the records to the S3 bucket as an S3 object. We might want to add a record separator at the end of each record before we send it to Kinesis Data Firehose. This will help us to divide a delivered Amazon S3 object into individual records.

Kinesis Data Firehose adds a UTC time prefix in the format YYYY/MM/dd/HH before writing objects to Amazon S3. This prefix creates a logical hierarchy in the bucket, where each forward slash (/) creates a level in the hierarchy. You can modify this structure by specifying a custom prefix. 

s3://reflectoring-pratik/2022/02/22/03/

## Analyze Streaming Data with Amazon Kinesis Data Analytics
The Amazon Kinesis Data Analytics service is used to analyze streaming data in real-time. It provides a fully managed service for running Apache Flink applications. 

Apache Flink is a Big Data processing framework for building applications that can process a large amount of data efficiently.

Amazon Kinesis Data Analytics sets up the resources to run Flink applications and scales automatically to handle any volume of incoming data.

### Structure of a Flink Application
A basic structure of a Flink application is shown below:

{{% image alt="Created Kinesis Delivery Stream" src="images/posts/aws-kinesis/flink-app-structure.png" %}}

1. **Execution Environment**: The execution environment of a Flink application is defined in the application main class and creates the data pipeline. The data pipeline contains the business logic and is composed of one or more operators chained together. 
2. **Data Source**: The application consumes data by using a source. A source connector reads data from a Kinesis data stream, an Amazon S3 bucket, etc. For more information, see Sources.
3. **Processing Operators**: The application processes data by using one or more operators. These processing operators apply transformations to the input data that comes from the data sources. After the transformation, the application forwards the transformed data to the data sinks. For more information, see DataStream API Operators.
4. **Data Sink**: The application produces data to external sources by using sinks. A sink connector writes data to a Kinesis data stream, a Kinesis Data Firehose delivery stream, an Amazon S3 bucket, etc. For more information, see Sinks.

### Creating a Flink Application

Let us first create a Flink application which we will run using the Kinesis Data Analytics service. We can create a a Flink application in Java, Scala or Python. We will create the application for our example as a Maven project in Java language and set up the following dependencies:

```xml
		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-streaming-java_2.11</artifactId>
			<version>1.14.3</version>
			<scope>provided</scope>
		</dependency>
```
The dependency `flink-streaming-java_2.11` contains the core API of Flink.

Let us use a stream of access logs from an Apache HTTP server as our streaming data that we will use for processing by our Flink application.

The code for processing this stream of access logs is shown below:

```java
public class ErrorCounter {
	private final static Logger logger = 
	  Logger.getLogger(ErrorCounter.class.getName());

	public static void main(String[] args) throws Exception {

        // set up the streaming execution environment
        final StreamExecutionEnvironment env = 
               StreamExecutionEnvironment.getExecutionEnvironment();

        // Create the source of streaming data
        DataStream<String> inputStream = createSource(env);

        // convert string to LogRecord event objects
        DataStream<LogRecord> logRecords = 
            inputStream.flatMap(new FlatMapFunction<String, LogRecord>() {

			@Override
			public void flatMap(String value, Collector<LogRecord> out) 
			  throws Exception {
				
				String[] parts = value.split("\\s+");

				LogRecord record = new LogRecord();
				record.setIp(parts[0]);
				record.setHttpStatus(parts[8]);
				record.setUrl(parts[6]);
				
				out.collect(record );

			}

		});
        
        // Filter out error records (with status not equal to 200)
        DataStream<LogRecord> errorRecords = 
          logRecords.filter(new FilterFunction<LogRecord>() {
			
			@Override
			public boolean filter(LogRecord value) throws Exception {

				boolean matched = !value.getHttpStatus()
				                   .equalsIgnoreCase("200");
				
				return matched;
			}
		});
        
        // Create keyed stream with IP as key and apply 
        // tumbling window of 5 seconds
        errorRecords
        .keyBy(value -> value.getIp())
        .window(TumblingEventTimeWindows.of(Time.seconds(5)));
        
        // Create sink
        createSink(env, errorRecords);
        
        // Execute the job
        env.execute("Error alerts");

	}
}

```

We have used the DataStream API of Flink in this code example for processing the streams of access logs.
Here we are first creating the execution environment using the `StreamExecutionEnvironment` class. Next, we are creating the source for the streaming data. A source can be as simple as a text file or a socket connection. For Kinesis we will connect to a Kinesis data stream.
After this, we will attach a set of operators. Our sample code uses the following operators:
1. Flatmap
2. Filter
3. KeyBy
4. Window

The result of the last operator is connected to a sink. In our case, the sink will be another Kinesis data stream.

After testing our data pipeline we will modify the `data source` in our code to connect to a Kinesis Data Stream which will ingest the streaming data which we want to process. For our example, it will be access logs from an Apache HTTP server ingested in a Kinesis Data Stream using an architecture as shown in this diagram:

{{% image alt="Automation with Kinesis" src="images/posts/aws-kinesis/s3-lambda-auto.png" %}}

We will need to package this code for submitting to Kinesis Data Analytics which will run this application in a scalable environment.

We will package our application using Maven as shown below:

```shell
mvn package -Dflink.version=1.13.2
```
Running this command will create a fat uber jar with all the dependencies. We will submit this jar file to the application in the Kinesis Data Analytics service as explained in the next section.

### Deploying to Kinesis Data Analytics
For deploying to Kinesis Data Analytics, we will first create an application using the administration console as shown below: 

{{% image alt="Create Kinesis Data Analytics Application" src="images/posts/aws-kinesis/kda-app-create.png" %}}

We will next configure this application with the location of the S3 bucket containing the Flink application that we created in the previous step. Before this, we will first upload the Flink application that we packaged into a Jar file to an S3 bucket. Our application's dependent resources like CloudWatch Logs streams and IAM service roles are also created. We also specify what version of Apache Flink your application uses.



### Running the Application

We can run our application by choosing `Run` on our application's page in the AWS console.
When we run our Kinesis Data Analytics application, the Kinesis Data Analytics service creates an Apache Flink job. The execution of the job, and the resources it uses, are managed by a Job Manager which separates the execution of the application into tasks. Each task is managed by a Task Manager. We examine the performance of each Task Manager, or the Job Manager as a whole to monitor the performance of our application.


### Creating Flink Application Interactively with Notebooks
Studio notebooks for Kinesis Data Analytics allow us to interactively query data streams in real-time and easily build and run stream processing applications using standard SQL, Python, and Scala. With a notebook, you model queries using the Apache Flink Table API & SQL in SQL, Python, Scala, or DataStream API in Scala. We can then promote the Studio notebook to a continuously-running, non-interactive, Kinesis Data Analytics stream-processing application.

## Amazon Kinesis Video Streams
Amazon Kinesis Video Streams is a fully managed AWS service that we can use to ingest streaming video, audio, and other time-encoded data from various capturing devices using an infrastructure provisioned dynamically in the AWS Cloud. The ingested data is stored durably and we can build applications to consume that data frame-by-frame, in real-time for low-latency processing.

Let us first create a Kinesis Video Stream using the AWS admin console:
{{% image alt="Create Kinesis Data Analytics Application" src="images/posts/aws-kinesis/create_video_stream.png" %}}
{{% image alt="Create Kinesis Data Analytics Application" src="images/posts/aws-kinesis/video_stream_created.png" %}}

Next, we need to configure a producer for putting data into this Kinesis Video Stream. A producer can be any video-generating device, such as a security camera, a body-worn camera, a smartphone camera, or a dashboard camera. We need to install Kinesis Video Streams Producer libraries (SDKs) on the producer and write code to enable it to send data into a Kinesis Video Stream. 

The Producer libraries contain the following components:

Kinesis Video Streams Producer Client
Kinesis Video Streams Producer Library

The Kinesis Video Streams Producer Client includes a single KinesisVideoClient class. This class manages media sources, receives data from the sources, and manages the stream lifecycle as data flows from a media source to Kinesis Video Streams. Furthermore, it provides a MediaSource interface for defining the interaction between Kinesis Video Streams and your proprietary hardware and software.

The Kinesis Video Streams Producer Client is available for Java and Android applications. For more information, see Using the Java Producer Library and Using the Android Producer Library.

### Key Concepts

Producer – Any source that puts data into a Kinesis video stream. A producer can be any video-generating device, such as a security camera, a body-worn camera, a smartphone camera, or a dashboard camera. A producer can also send non-video data, such as audio feeds, images, or RADAR data.

Kinesis video stream – A resource that enables you to transport live video data, optionally store it and make the data available for consumption both in real-time and on a batch or ad hoc basis.

Consumer – Gets data, such as fragments and frames, from a Kinesis video stream to view, process, or analyze it. Generally, these consumers are called Kinesis Video Streams applications.

### Creating a Kinesis Video Stream


### Sending Data to a Kinesis Video Stream


### Consuming Media Data
We can consume media data by either viewing it in the console or by creating an application that reads media data from a stream using HLS.





## Conclusion

Here is a list of the major points for a quick reference:

Great, we just learned how we can use Kinesis data analytics to get real time insights in our streamed data. In our case it gave us the possibility to get an on-demand view of the traffic jams in Belgium and send out alerts for emerging traffic jams. Below are the key takeaways from this article so you can go ahead and create your own real-time application!

Separate cold and hot flow of your data completely (real time vs batch).
Real time date should be produced in real time and arrive in (near)real time.
Think about your data access pattern upfront.
Mind the differences between Kinesis Firehose and Kinesis Data Streams to stream your data.
Preprocess and/or filter your records before they go in your Kinesis analytics application by using a record preprocessor Lambda Function.
You can use Windowing to aggregate or correlate results over a certain timespan.
You can only use the timestamps ROWTIME and APPROXIMATE_ARRIVAL_TIME.
Add static reference data to your application by making it available via S3.
The core SQL concepts of the Kinesis analytics app are SQL STREAMS and SQL PUMPS.


You can refer to all the source code used in the article on [Github](https://github.com/thombergs/code-examples/tree/master/aws/kinesis).

