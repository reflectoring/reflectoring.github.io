---
authors: [pratikdas]
title: "Processing Streams with Amazon Kinesis"
categories: ["aws"]
date: 2022-03-03T00:00:00 
excerpt: "Amazon Kinesis is a family of managed services for collecting and processing
streaming data in real-time. Stream processing platforms are an integral part of the big data ecosystem. In this article, we will introduce Amazon Kinesis and understand its core concepts of creating data streams, sending, and receiving data from streams, and deriving analytical insights using the different services of the family: Kinesis Data Stream, Kinesis Data Firehose, Kinesis Data Analytics, and Kinesis Video Streams."
image: images/stock/0115-2021-1200x628-branded.jpg 
url: getting-started-with-aws-kinesis
---

Amazon Kinesis is a family of managed services for **collecting and processing streaming data in real-time**. Stream processing platforms are an integral part of the Big Data ecosystem.

Examples of streaming data are data collected from website click-streams, marketing, and financial information, social media feeds, IoT sensors, and monitoring and operational logs.

In this article, we will introduce Amazon's Kinesis services and understand their core concepts of creating, storing, and receiving data streams and deriving analytical insights by working through some examples.

{{% github "https://github.com/thombergs/code-examples/tree/master/aws/kinesis" %}}

## What is Streaming Data?

Streaming data is data that is generated continuously (in a stream) by multiple data sources which typically send the data records simultaneously. Due to its continuous nature, streaming data is also called unbounded data as opposed to bounded data handled by batch processing systems.

Streaming data includes a wide variety of data such as log files generated by customers using mobile or web applications, e-commerce purchases, in-game player activity, information from social networks, financial trading floors, or geospatial services, and telemetry from connected devices or instrumentation in data centers.

Streaming data is processed sequentially and incrementally either by one record at a time or in batches of records aggregated over sliding time windows.

## What is Amazon Kinesis?

Amazon Kinesis is a fully managed streaming data platform for processing streaming data. It provides four specialized services based on the type of stream data processing we want to perform to suit our use case:

- **Kinesis Data Streams**: The Kinesis Data Streams service is used to capture streaming data produced by various data sources in real-time. Producer applications write to the Kinesis Data Stream and consumer applications connected to the stream read the data for different types of processing.

- **Kinesis Data Firehose**: With Kinesis Data Firehose, we do not need to write applications or manage resources. We configure data producers to send data to Kinesis Data Firehose, and it automatically delivers the data to the specified destination. We can also configure Kinesis Data Firehose to transform the data before delivering it.

- **Kinesis Data Analytics**: With Amazon Kinesis Data Analytics we can process and analyze streaming data. It provides an efficient and scalable environment to run Flink applications.

- **Kinesis Video Streams**: Amazon Kinesis Video Streams is a fully managed AWS service that we can use to stream live media from video or audio capturing devices to the AWS Cloud, or build applications for real-time video processing or batch-oriented video analytics.

Let us understand these services in the next sections. In each section, we will first introduce the key concepts of the service and then work through some examples.

## Kinesis Data Streams - Ingesting and Storing Streaming Data

The Kinesis Data Streams service is used to collect and store streaming data as soon as it is produced (in real-time). The stored data is processed in real or near real-time by custom-built applications running on compute services like EC2, EMR, or Lambda. 

This makes the Kinesis Data Streams service most useful for building time-sensitive applications like real-time dashboards, and anomaly detection. Another common use of Kinesis Data Streams is the real-time aggregation of data followed by loading the aggregated data into a data warehouse or map-reduce cluster.

The data is stored for 24 hours by default which can be configured to store the data beyond 24 hours up to 365 days.

### Kinesis Data Stream, Shards, and Records

When using Kinesis Data Streams, we first set up a data stream and then build producer applications that write data to the data stream and consumer applications that read and process the data from the data stream in real-time:

{{% image alt="Create Kinesis Delivery Stream" src="images/posts/aws-kinesis/kds-shards.png" %}}

The Kinesis Data Stream is composed of multiple data carriers called shards, as we can see in this diagram. Each shard provides a fixed unit of capacity. The data capacity of a data stream is a function of the number of shards in the stream. The total capacity of the data stream is the sum of the capacities of all the shards it is composed of.

The data stored in the shard is called a record.

Each shard contains a sequence of data records. Each data record has a sequence number that is assigned by Kinesis Data Streams.

### Creating a Kinesis Data Stream

Let us first create our data stream where we can send our data.

We can create a Kinesis data stream either by using the AWS Kinesis Data Streams Management Console, from the AWS CLI or using the `CreateStream` operation of AWS SDK. 

We can also use [AWS CloudFormation](https://reflectoring.io/getting-started-with-aws-cloudformation/) or [AWS CDK](https://reflectoring.io/getting-started-with-aws-cdk/) to create a data stream as part of an infrastructure-as-code project.

Our code for creating a data stream with AWS SDK for Java looks like this:

```java
public class DataStreamResourceHelper {

  public static void createDataStream() {
    KinesisClient kinesisClient = getKinesisClient();

    CreateStreamRequest createStreamRequest
            = CreateStreamRequest
            .builder()
            .streamName(Constants.MY_DATA_STREAM)
            .streamModeDetails(
                    StreamModeDetails
                            .builder()
                            .streamMode(StreamMode.ON_DEMAND)
                            .build())
            .build();

    CreateStreamResponse createStreamResponse
            = kinesisClient.createStream(createStreamRequest);
                ...
                ...

  }

  private static KinesisClient getKinesisClient() {
    AwsCredentialsProvider credentialsProvider =
            ProfileCredentialsProvider
                    .create(Constants.AWS_PROFILE_NAME);

    KinesisClient kinesisClient = KinesisClient
            .builder()
            .credentialsProvider(credentialsProvider)
            .region(Region.US_EAST_1).build();
    return kinesisClient;
  }
}
```

In the code snippet, we are creating a Kinesis Data Stream with `ON_DEMAND` capacity mode. The capacity mode of `ON_DEMAND` is used for unpredictable workloads which scale the capacity of the data stream automatically in response to varying data traffic.

With the Kinesis Data Stream created, we will look at how to add data to this stream in the next section.

### Structure of Data Added to Kinesis Data Stream

Before adding data, it is also important to understand the structure of data that is added to a Kinesis Data Stream.

**Data is written to a Kinesis Data Stream as a record.**

A record in a Kinesis data stream consists of:

- a sequence number,
- a partition key, and 
- a data blob. 

The maximum size of a data blob (the data payload before Base64-encoding) is 1 megabyte (MB).

**A sequence number is a unique identifier for each record.** The sequence number is assigned by Amazon Kinesis Data Streams when a producer application calls the `PutRecord()` or `PutRecords()` operation to add data to a Kinesis Data Stream.

**The partition key is used to segregate and route records to different shards of a stream.** We need to specify the partition key in the producer application while adding data to a Kinesis stream.

For example, if we have a stream with two shards: `shard1` and `shard2`, we can write our producer application to use two partition keys: `key1` and `key2` so that all records with `key1` are added to `shard1` and all records with `key2` are added to `shard2`.

### Data Ingestion - Writing Data to Kinesis Data Streams

Applications that write data to Kinesis Data Streams are called "producers". Producer applications can be custom-built in a supported programming language using AWS SDK or by using the Kinesis Producer Library (KPL).

We can also use Kinesis Agent which is a stand-alone application that we can run as an agent on Linux-based server environments such as web servers, log servers, and database servers.

Let us create a producer application in Java that will use the AWS SDK's `PutRecord()` operation for adding a single record and the `PutRecords()` operation for adding multiple records to the Kinesis Data Stream.

We have created this producer application as a Maven project and added a Maven dependency in our `pom.xml` as shown below:

```xml
<dependencies>
    <dependency>
        <groupId>software.amazon.awssdk</groupId>
        <artifactId>kinesis</artifactId>
    </dependency>
</dependencies>
<dependencyManagement>
  <dependencies>
      <dependency>
          <groupId>software.amazon.awssdk</groupId>
          <artifactId>bom</artifactId>
          <version>2.17.116</version>
          <type>pom</type>
          <scope>import</scope>
      </dependency>
  </dependencies>
</dependencyManagement>
```

In the `pom.xml`, we have added the `kinesis` library as a dependency after adding `bom` for AWS SDK.

Here is the code for adding a single event to the Kinesis Data Stream that we created in the previous step:

```java
public class EventSender {

  private static final Logger logger = Logger
          .getLogger(EventSender.class.getName());

  public static void main(String[] args) {
    sendEvent();
  }

  public static void sendEvent() {
    KinesisClient kinesisClient = getKinesisClient();

    // Set the partition key
    String partitionKey = "partitionKey1";

    // Create the data to be sent to Kinesis Data Stream in bytes
    SdkBytes data = SdkBytes.fromByteBuffer(
            ByteBuffer.wrap("Test data".getBytes()));

    // Create the request for putRecord method
    PutRecordRequest putRecordRequest
            = PutRecordRequest
            .builder()
            .streamName(Constants.MY_DATA_STREAM)
            .partitionKey(partitionKey)
            .data(data)
            .build();

    // Call the method to write the record to Kinesis Data Stream
    PutRecordResponse putRecordsResult
            = kinesisClient.putRecord(putRecordRequest);

    logger.info("Put Result" + putRecordsResult);
    kinesisClient.close();
  }

  private static KinesisClient getKinesisClient() {
    AwsCredentialsProvider credentialsProvider =
            ProfileCredentialsProvider
                    .create(Constants.AWS_PROFILE_NAME);

    KinesisClient kinesisClient = KinesisClient
            .builder()
            .credentialsProvider(credentialsProvider)
            .region(Region.US_EAST_1).build();
    return kinesisClient;
  }
}
```

Here we are first creating the request object for the `putRecord()` method with the name of the Kinesis Data Stream, partition key, and the data to be sent in bytes. Then we have invoked the `putRecord()` method on the `kinesisClient` to add a record to the stream.

Running this program gives the following output:

```shell
INFO: Put ResultPutRecordResponse(ShardId=shardId-000000000001, SequenceNumber=49626569155656830268862440193769593466823195675894743058)
```

We can see the `shardId` identifier of the shard where the record is added along with the sequence number of the record.

Let us next add multiple events to the Kinesis data stream by using the `putRecords()` method as shown below:

```java
public class EventSender {

  private static final Logger logger =
          Logger.getLogger(EventSender.class.getName());

  public static void main(String[] args) {
    sendEvents();
  }

  public static void sendEvents() {
    KinesisClient kinesisClient = getKinesisClient();

    String partitionKey = "partitionKey1";

    List<PutRecordsRequestEntry> putRecordsRequestEntryList
            = new ArrayList<>();

    // Create 5 records for adding to the Kinesis Data Stream
    for (int i = 0; i < 5; i++) {
      SdkBytes data = ...
      PutRecordsRequestEntry putRecordsRequestEntry = ...
      putRecordsRequestEntryList.add(putRecordsRequestEntry);
    }

    // Create the request for putRecords method
    PutRecordsRequest putRecordsRequest
            = PutRecordsRequest
            .builder()
            .streamName(Constants.MY_DATA_STREAM)
            .records(putRecordsRequestEntryList)
            .build();

    PutRecordsResponse putRecordsResult = kinesisClient
            .putRecords(putRecordsRequest);

    logger.info("Put records Result" + putRecordsResult);
    kinesisClient.close();
  }

  private static KinesisClient getKinesisClient() {
      ...
      ...
  }
}
```

Here we are first creating the request object for the `putRecords()` method with the name of the data stream, partition key, and sequence number. Running this program gives the following output:

```shell
ResultPutRecordsResponse(FailedRecordCount=0, 
    Records=[
    PutRecordsResultEntry(SequenceNumber=49626569155656830268862440193770802392642928158972051474, ShardId=shardId-000000000001), 
    PutRecordsResultEntry(SequenceNumber=49626569155656830268862440193772011318462542788146757650, ShardId=shardId-000000000001), 
    PutRecordsResultEntry(SequenceNumber=49626569155656830268862440193773220244282157417321463826, ShardId=shardId-000000000001), 
    PutRecordsResultEntry(SequenceNumber=49626569155656830268862440193774429170101772046496170002, ShardId=shardId-000000000001), 
    PutRecordsResultEntry(SequenceNumber=49626569155656830268862440193775638095921386675670876178, ShardId=shardId-000000000001)])
```

In this output, we can see the same `shardId` for all the records added to the stream. This is because we are using the same partition key for all our records, they have been put into the same shard with shardId : `shardId-000000000001`.

As mentioned before, other than the AWS SDK, we can use the Kinesis Producer Library (KPL) or the Kinesis agent for adding data to a Kinesis Data Stream:

1. **Kinesis Producer Library (KPL)**: KPL is a library written in C++ for adding data into a Kinesis data stream. It runs as a child process to the main user process. So in case the child process stops due to an error when connecting or writing to a Kinesis Data Stream, the main process continues to run.

2. **Amazon Kinesis Agent**: Kinesis Agent is a stand-alone application that we can run as an agent on Linux-based server environments such as web servers, log servers, and database servers. The agent continuously monitors a set of files and collects and sends new data to Kinesis Data Streams. Please refer to the official documentation for guidance on configuring Kinesis Agent in a Linux-based server environment.

### Data Consumption - Reading Data from Kinesis Data Streams

With the data ingested in our data stream, let us get into creating a consumer application that can process the data from this data stream. 

Earlier we had created a Kinesis Data Stream and added streaming data to it using the `putRecord()`  and `putRecords()` operation of the Kinesis Data Streams API. We can also use the [Kinesis Data Streams API](https://docs.aws.amazon.com/kinesis/latest/APIReference/Welcome.html) from the [AWS SDK](https://aws.amazon.com/tools/) for reading the streaming data from this Kinesis Data Stream as shown below:

```java
public class EventConsumer {

  public static void receiveEvents() {
    KinesisClient kinesisClient = getKinesisClient();

    String shardId = "shardId-000000000001";

    // Prepare the shard iterator request with the stream name 
    // and identifier of the shard to which the record was written
    GetShardIteratorRequest getShardIteratorRequest
            = GetShardIteratorRequest
            .builder()
            .streamName(Constants.MY_DATA_STREAM)
            .shardId(shardId)
            .shardIteratorType(ShardIteratorType.TRIM_HORIZON.name())
            .build();

    GetShardIteratorResponse getShardIteratorResponse
            = kinesisClient
            .getShardIterator(getShardIteratorRequest);
    
    // Get the shard iterator from the Shard Iterator Response
    String shardIterator = getShardIteratorResponse.shardIterator();

    while (shardIterator != null) {
      // Prepare the get records request with the shardIterator
      GetRecordsRequest getRecordsRequest
              = GetRecordsRequest
              .builder()
              .shardIterator(shardIterator)
              .limit(5)
              .build();

      // Read the records from the shard
      GetRecordsResponse getRecordsResponse
              = kinesisClient.getRecords(getRecordsRequest);

      List<Record> records = getRecordsResponse.records();

      logger.info("count " + records.size());

      // log content of each record
      records.forEach(record -> {
        byte[] dataInBytes = record.data().asByteArray();
        logger.info(new String(dataInBytes));
      });

      shardIterator = getRecordsResponse.nextShardIterator();
    }

    kinesisClient.close();
  }

  // set up the Kinesis Client
  private static KinesisClient getKinesisClient() {
    AwsCredentialsProvider credentialsProvider =
            ProfileCredentialsProvider
                    .create(Constants.AWS_PROFILE_NAME);

    KinesisClient kinesisClient = KinesisClient
            .builder()
            .credentialsProvider(credentialsProvider)
            .region(Region.US_EAST_1).build();

    return kinesisClient;
  }

}
```
Here we are invoking the `getRecords()` method on the Kinesis client to read the ingested records from the Kinesis Data Stream. We have provided a shard iterator using the `ShardIterator` parameter in the request. 

The shard iterator specifies the position in the shard from which we want to start reading the data records sequentially. We will get an empty list, if there are no records available in the portion of the shard that the iterator is pointing to so we have used a `while` loop to make multiple calls to get to the portion of the shard that contains records.

### Type of Consumers of Kinesis Data Streams
Kinesis Data Streams API is a low-level method of reading streaming data. We have to take care of polling the stream, checkpointing processed records, running multiple instances, etc. So in most practical situations, we use the following methods for creating consumer applications for reading data from the stream:

1. **AWS Lambda**: We can use an [AWS Lambda](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html) function to process records in an Amazon Kinesis data stream. AWS Lambda integrates natively with Amazon Kinesis Data Stream as a consumer to process data ingested through a data stream by taking care of the polling, checkpointing, and error handling functions. Please refer to the [AWS Lambda documentation](https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html) for the steps to configure a Lambda function as a consumer to a Kinesis Data Stream.

2. **Kinesis Client Library (KCL)**: We can build a consumer application for Amazon Kinesis Data Streams using the Kinesis Client Library (KCL). The KCL is different from the Kinesis Data Streams API used earlier. It provides a layer of abstraction around the low-level tasks like connecting to the stream, reading the record from the stream, checkpointing processed records, and reacting to resharding. For information on using KCL, check the [documentation](https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-kcl-v2.html) for developing KCL consumers.

3. **Kinesis Data Firehose**: Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and Splunk. We can set up the Kinesis Data Stream as a source of streaming data to a Kinesis Firehose delivery stream for delivering after optionally transforming the data to a configured destination. We will explain this mechanism further in the section on Kinesis Data Firehose.

4. **Kinesis Data Analytics**:Kinesis Data Analytics is another fully managed service from the Kinesis family for processing and analyzing streaming data with helpful programming constructs like windowing, sorting, filtering, etc. We can set up the Kinesis Data Stream as a source of streaming data to a Kinesis Data Analytics application which we will explain in the section on Kinesis Data Analytics.

### Throughput Limits of Kinesis Data Stream - Shared vs Enhanced Fan-Out Consumers

It is important to understand the throughput limits of Kinesis Data Stream for designing and operating a highly reliable data streaming workflow and ensuring predictable performance.

As explained before, the Kinesis Data Stream is composed of multiple data carriers called shards which contain a sequence of data records. Each shard provides a fixed unit of capacity thereby serving as a base throughput unit of a Kinesis data stream. The data capacity of a data stream is a function of the number of shards in the stream. 

**A shard supports 1 MB/second and 1,000 records per second for write throughput and 2 MB/second for read throughput.**

When we have multiple consumers reading from a shard, this read throughput is shared between them. This type of consumers are called Shared fan-out consumers.

If we want dedicated throughput for consumers, we can define them as Enhanced fan-out consumers.

Enhanced fan-out is an optional feature for Kinesis Data Streams consumers that provides dedicated 2 MB/second throughput between consumers and shards. This helps to scale the number of consumers reading from a data stream in parallel while maintaining high performance. 

These consumers do not have to share this throughput with other consumers that are receiving data from the stream. The data records from the stream are also pushed to these consumers that use enhanced fan-out.

The producer and consumer applications will receive throttling errors when writes and reads exceed the shard limits, which are handled through retries.

## Kinesis Data Firehose - Delivering Streaming Data to Destinations in Near Real-Time

Kinesis Data Firehose is a fully managed service that is used to deliver streaming data to a destination in near real-time.

One or more data producers send their streaming data into a kind of "pipe" called a delivery stream which optionally applies some transformation to the data before delivering it to a destination.

{{% image alt="Create Kinesis Data Firehose Delivery Stream" src="images/posts/aws-kinesis/firehose.png" %}}

The incoming streaming data is buffered in the delivery stream till it reaches a particular size or exceeds a certain time interval before it is delivered to the destination. Due to this reason, Kinesis Data Firehose is not intended for real-time delivery. Rather, it will batch incoming messages, optionally compress and/or transform them with AWS Lambda, and then sink data, usually into an AWS service like S3, Redshift, or Elasticsearch.

We also need to do very little programming when using Firehose. This is unlike Kinesis Data Streams where we write custom applications for producers and consumers.

An example of a data producer is a web server that sends log data to a delivery stream.

We can also configure the delivery stream to read streaming data from a Kinesis Data Stream and deliver it to a destination.

### Creating a Kinesis Firehose Delivery Stream

Let us take a closer look at the Kinesis Data Firehose service by creating a Firehose delivery stream. We can create a Firehose delivery stream using the AWS console, AWS SDK, or infrastructure as a service like [AWS CloudFormation](https://reflectoring.io/getting-started-with-aws-cloudformation/) and [AWS CDK](https://reflectoring.io/getting-started-with-aws-cdk/).

For our example, let us use the AWS console for creating the delivery stream as shown below:

{{% image alt="Create Kinesis Delivery Stream" src="images/posts/aws-kinesis/kdf_create.png" %}}

We configure a delivery stream in Firehose with a source and a destination.

The source of a Kinesis Data Firehose delivery stream can be :

1. A Kinesis Data Stream
2. `Direct PUT` which means an application producer can send data to the delivery stream using a direct `PUT` operation.

Here we have chosen the source as `Direct PUT`.

Similarly, the delivery stream can send data to the following destinations:

1. Amazon Simple Storage Service (Amazon S3),
2. Amazon Redshift
3. Amazon OpenSearch Service
4. Splunk, and
5. Any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers like Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, and Sumo Logic.

We have chosen our destination as `S3` and configured an S3 bucket that will receive the streaming data delivered by our Firehose delivery stream.

Apart from this we also need to assign an IAM service role to the Kinesis Firehose service with an access policy that allows it to write to the S3 bucket.

The delivery stream which we created using this configuration looks like this:

{{% image alt="Created Kinesis Delivery Stream" src="images/posts/aws-kinesis/kdf_created.png" %}}

In this screenshot, we can observe a few more properties of the delivery stream like `Data transformation` and `Dynamic partitioning` which we will understand in the subsequent sections. We can also see the status of the delivery stream as `Active` which means it can receive streaming data. The initial status of the delivery stream is CREATING.

We are now ready to send streaming data to our Firehose delivery stream which will deliver this data to the configured destination.

### Sending Data to a Kinesis Firehose Delivery Stream

As explained earlier, a Firehose delivery stream can receive data from two kinds of sources: a Kinesis delivery stream or from an application producing data. We have already seen how to put data to a Kinesis data stream. If we connect this data stream to a Kinesis firehose delivery stream, data will be automatically sent to the destination.

We created our firehose delivery stream in the previous section with the `Direct PUT` option due to which we can send data directly to the Firehose delivery stream from a producer application.

A very simplified code snippet for sending a single record to a Kinesis Firehose delivery stream looks like this:

```java
public class FirehoseEventSender {
  private final static Logger logger =
          Logger.getLogger(FirehoseEventSender.class.getName());

  public static void main(String[] args) {
    new FirehoseEventSender().sendEvent();
  }

  public void sendEvent() {
    String deliveryStreamName = "PUT-S3-5ZGgA";

    String data = "Test data" + "\n";

    // Create a record for sending to Firehose Delivery Stream
    Record record = Record
            .builder()
            .data(SdkBytes
                    .fromByteArray(data.getBytes()))
            .build();

    PutRecordRequest putRecordRequest =
            PutRecordRequest
                    .builder()
                    .deliveryStreamName(deliveryStreamName)
                    .record(record)
                    .build();

    FirehoseClient firehoseClient = getFirehoseClient();

    // Put record into the DeliveryStream
    PutRecordResponse putRecordResponse =
            firehoseClient.putRecord(putRecordRequest);

    logger.info("record ID:: " + putRecordResponse.recordId());

    firehoseClient.close();
  }

  private static FirehoseClient getFirehoseClient() {
    AwsCredentialsProvider credentialsProvider =
            ProfileCredentialsProvider
                    .create(Constants.AWS_PROFILE_NAME);

    FirehoseClient kinesisClient = FirehoseClient
            .builder()
            .credentialsProvider(credentialsProvider)
            .region(Constants.AWS_REGION).build();
    return kinesisClient;
  }
}
```

Here we are calling the `putRecord()` method for adding a single record to the delivery stream.

The Kinesis Data Firehose API offers two operations for sending data to the Firehose delivery stream: `PutRecord()` and `PutRecordBatch()`. `PutRecord()` sends one data record within one call and `PutRecordBatch()` can send multiple data records within one call.

Other than this, we can send streaming data to a Delivery Stream using Firehose Agent.

### Data Transformation in a Firehose Delivery Stream

We can configure the Kinesis Data Firehose delivery stream to transform incoming source data and deliver the transformed data to destinations by attaching a lambda function.

Kinesis Data Firehose can invoke a Lambda function to transform incoming source data and deliver the transformed data to destinations. We can enable Kinesis Data Firehose data transformation when we create our delivery stream.

### Data Delivery Format of a Firehose Delivery Stream

After our delivery stream receives the streaming data, it is automatically delivered to the configured destination. Each destination type supported by Firehose has specific configurations for data delivery.

For data delivery to S3, Kinesis Data Firehose concatenates multiple incoming records based on the buffering configuration of our delivery stream. It then delivers the records to the S3 bucket as an S3 object. We can also add a record separator at the end of each record before sending them to Kinesis Data Firehose. This will help us to divide the delivered Amazon S3 object into individual records.

Kinesis Data Firehose adds a [UTC](https://en.wikipedia.org/wiki/Coordinated_Universal_Time) time prefix in the format `YYYY/MM/dd/HH` before writing objects to an S3 bucket. This prefix creates a logical hierarchy in the bucket, where each `/` creates a level in the hierarchy.

The path of our S3 object created as a result of delivery of our streaming data with the `Direct PUT` operation results in a hierarchy of the form `s3://reflectoring-pratik/2022/02/22/03/` in the S3 bucket configured as a destination to the Firehose delivery stream.

The data delivery format of other destinations can be found in the [official documentation](https://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html).

## Kinesis Data Streams vs. Kinesis Data Firehose

## Kinesis Data Analytics - Analyzing Streaming Data in Real-Time

Kinesis Data Analytics provides a fully managed environment for running applications which read streaming data from a source like Kinesis Data Stream for performing analytic operations like windowing, filtering, aggregations, mapping, etc on the streaming data in real-time. The results of processing are used in various real-time analytics use cases. 

The applications run by Kinesis Data Analytics are built using Apache Flink which is a Big Data processing framework for processing a large amount of data efficiently.

Kinesis Data Analytics sets up the resources to run Flink applications and scales automatically to handle any volume of incoming data.

Kinesis Data Analytics applications continuously read and process streaming data in real-time. We write application code in a language supported by the Apache Flink framework to process the incoming streaming data and produce an output that Kinesis Data Analytics writes to a configured destination.

Kinesis Data Analytics also supports applications built using Java with the open-source [Apache Beam](https://beam.apache.org/documentation/programming-guide/) libraries and our own custom code.

### Structure of a Flink Application

A basic structure of a Flink application is shown below:

{{% image alt="Created Kinesis Delivery Stream" src="images/posts/aws-kinesis/flink-app-structure.png" %}}

In this diagram, we can observe the following components of the Flink application:

1. **Execution Environment**: The execution environment of a Flink application is defined in the application main class and creates the data pipeline. The data pipeline contains the business logic and is composed of one or more operators chained together.
2. **Data Source**: The application consumes data by using a source. A source connector reads data from a Kinesis data stream, an Amazon S3 bucket, etc.
3. **Processing Operators**: The application processes data by using one or more operators. These processing operators apply transformations to the input data that comes from the data sources. After the transformation, the application forwards the transformed data to the data sinks. Please check out the Flink documentation to see the complete list of [DataStream API Operators](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/overview/) with code snippets.
4. **Data Sink**: The application produces data to external sources by using sinks. A sink connector writes data to a Kinesis data stream, a Kinesis Data Firehose delivery stream, an Amazon S3 bucket, etc.

A few basic data sources and sinks are built into Flink and are always available. Examples of predefined data sources are reading from files, and sockets, and ingesting data from collections and iterators. Similarly, examples of predefined data sink include writing to files, to stdout and stderr, and sockets.

### Creating a Flink Application

Let us first create a Flink application which we will run using the Kinesis Data Analytics service. We can create a Flink application in Java, Scala or Python. We will create the application for our example as a Maven project in Java language and set up the following dependencies:

```xml
<dependencies>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-streaming-java_2.11</artifactId>
        <version>1.14.3</version>
        <scope>provided</scope>
    </dependency>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-clients_2.11</artifactId>
        <version>1.14.3</version>
    </dependency>
    <dependency>
        <groupId>software.amazon.kinesis</groupId>
        <artifactId>amazon-kinesis-connector-flink</artifactId>
        <version>2.3.0</version>
    </dependency>
    ...
</dependencies>
```

The dependency: `flink-streaming-java_2.11` contains the core API of Flink. We have added the `flink-clients_2.11` dependency for running the Flink application locally. For connecting to Kinesis we are using the dependency: `amazon-kinesis-connector-flink`.

Let us use a stream of access logs from an [Apache HTTP server](https://httpd.apache.org/) as our streaming data that we will use for processing by our Flink application. We will first test our application using a text file as a pre-defined source and stdout as a pre-defined sink.

The code for processing this stream of access logs is shown below:

```java
public class ErrorCounter {
  private final static Logger logger =
          Logger.getLogger(ErrorCounter.class.getName());

  public static void main(String[] args) throws Exception {

    // set up the streaming execution environment
    final StreamExecutionEnvironment env =
            StreamExecutionEnvironment
                    .getExecutionEnvironment();

    // Create the source of streaming data
    DataStream<String> inputStream = createSource(env);
    
    // convert string to LogRecord event objects
    DataStream<LogRecord> logRecords = mapStringToLogRecord(inputStream);

    // Filter out error records (with status not equal to 200)
    DataStream<LogRecord> errorRecords = filterErrorRecords(logRecords);
    
    // Create keyed stream with IP as key
    DataStream<LogRecord> keyedStream = assignIPasKey(errorRecords);
    
    // convert LogRecord to string to objects
    DataStream<String> keyedStreamAsText = mapLogRecordToString(keyedStream);

    // Create sink
    createSink(env, errorRecords);

    // Execute the job
    env.execute("Error alerts");

  }

  // convert LogRecord to string to objects using the Flink's flatMap operator
  private static DataStream<String> mapLogRecordToString(
    DataStream<LogRecord> keyedStream) {
        DataStream<String> keyedStreamAsText 
        = keyedStream.flatMap(new FlatMapFunction<LogRecord, String>() {

            @Override
            public void flatMap(
                LogRecord value, 
                Collector<String> out) throws Exception {
                out.collect(value.getUrl()+"::" + value.getHttpStatus());
            }
        });
        return keyedStreamAsText;
  }

  // Create keyed stream with IP as key using Flink's keyBy operator
  private static DataStream<LogRecord> assignIPasKey(
    DataStream<LogRecord> errorRecords) {
        DataStream<LogRecord> keyedStream = 
            errorRecords.keyBy(value -> value.getIp());
        return keyedStream;
  }


  // Filter out error records (with status not equal to 200)
  // using Flink's filter operator
  private static DataStream<LogRecord> filterErrorRecords(
    DataStream<LogRecord> logRecords) {
        DataStream<LogRecord> errorRecords = 
        logRecords.filter(new FilterFunction<LogRecord>() {

            @Override
            public boolean filter(LogRecord value) throws Exception {
                boolean matched = !value.getHttpStatus().equalsIgnoreCase("200");
                return matched;
            }
        });
        return errorRecords;
  }


  // convert string to LogRecord event objects using Flink's flatMap operator
  private static DataStream<LogRecord> mapStringToLogRecord(
    DataStream<String> inputStream) {
        DataStream<LogRecord> logRecords = 
        inputStream.flatMap(new FlatMapFunction<String, LogRecord>() {

            @Override
            public void flatMap(
                String value, 
                Collector<LogRecord> out) throws Exception {

                String[] parts = value.split("\\s+");

                LogRecord record = new LogRecord();
                record.setIp(parts[0]);
                record.setHttpStatus(parts[8]);
                record.setUrl(parts[6]);

                out.collect(record);

            }

        });
        return logRecords;
    }

  // Set up the text file as a source
  private static DataStream<String> createSource(
          final StreamExecutionEnvironment env) {

    return env.readTextFile(
            "<File Path>/apache_access_log");
  }

  // Set up stdout as the sink
  private static void createSink(
          final StreamExecutionEnvironment env,
          DataStream<LogRecord> input) {

    input.print();
  }
}
```

We have used the [DataStream API](https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/overview/) of Flink in this code example for processing the streams of access logs contained in the text file used as a source. Here we are first creating the execution environment using the `StreamExecutionEnvironment` class. 

Next, we are creating the source for the streaming data. We have used a [text file](https://github.com/thombergs/code-examples/tree/master/aws/kinesis/src/main/resources/apache_access_log) containing the log records from an Apache HTTP server as a source here. Some sample log records from the text file are shown here:

```shell
83.149.9.216 .. "GET /.../-search.png HTTP/1.1" 200 ..
83.149.9.216 .. "GET /.../-dashboard3.png HTTP/1.1" 200 
83.149.9.216 .. "GET /.../highlight.js HTTP/1.1" 403 ..
...
...
```

After this, we have attached a chain of Flink operators to the source of the streaming data. Our sample code uses operators chained together in the below sequence:

1. **Flatmap:** We are using the `Flatmap` operator to transform the String element to a POJO of type `LogRecord`. FlatMap functions take elements and transform them, into zero, one, or more elements.
2. **Filter:** We have applied the `Filter` operator to select only the error records with HTTP status not equal to `200`.
3. **KeyBy:** With the `KeyBy` operator we partition the records by IP address for parallel processing.
4. **Flatmap:** We are once again using another `Flatmap` operator to transform the POJO of type `LogRecord` to a `String` element.

The result of the last operator is connected to a predefined sink: `stdout`.

Here is the output after running this application:

```shell
6> /doc/index.html?org/elasticsearch/action/search/SearchResponse.html::404
4> /presentations/logstash-monitorama-2013/css/fonts/Roboto-Bold.ttf::404
4> /presentations/logstash-monitorama-2013/images/frontend-response-codes.png::310
```

### Configuring a Kinesis Data Stream as a Source and a Sink

After testing our data pipeline we will modify the `data source` in our code to connect to a Kinesis Data Stream which will ingest the streaming data which we want to process. For our example, it will be access logs from an Apache HTTP server ingested in a Kinesis Data Stream using an architecture as shown in this diagram:

{{% image alt="Automation with Kinesis" src="images/posts/aws-kinesis/s3-lambda-auto.png" %}}

In this architecture, the access log files from the HTTP server will be uploaded to an S3 bucket. A lambda trigger attached to the S3 bucket will read the records from the file and add them to a Kinesis Data Stream using the `putRecords()` operation.

The source and the sink connected to Kinesis Data Stream looks like this:

```java
public class ErrorCounter {
  private final static Logger logger =
          Logger.getLogger(ErrorCounter.class.getName());

  public static void main(String[] args) throws Exception {
    // set up the streaming execution environment
    final StreamExecutionEnvironment env =
            StreamExecutionEnvironment
                    .getExecutionEnvironment();

    DataStream<String> inputStream = createSource(env);
        ...
        ...
        ...
    DataStream<String> keyedStream = ...
    keyedStream.addSink(createSink());
  }

  // Create Kinesis Data Stream as a source
  private static DataStream<String> createSource(
          final StreamExecutionEnvironment env) {

    Properties inputProperties = new Properties();
    inputProperties.setProperty(
            ConsumerConfigConstants.AWS_REGION,
            Constants.AWS_REGION.toString());

    inputProperties.setProperty(
            ConsumerConfigConstants.STREAM_INITIAL_POSITION,
            "LATEST");

    String inputStreamName = "in-app-log-stream";

    return env.addSource(
            new FlinkKinesisConsumer<>(
                    inputStreamName,
                    new SimpleStringSchema(),
                    inputProperties));
  }

  // Create Kinesis Data Stream as a sink
  private static FlinkKinesisProducer<String> createSink() {
    Properties outputProperties = new Properties();

    outputProperties.setProperty(
            ConsumerConfigConstants.AWS_REGION,
            Constants.AWS_REGION.toString());

    FlinkKinesisProducer<String> sink =
            new FlinkKinesisProducer<>(
                    new SimpleStringSchema(), outputProperties);

    String outputStreamName = "log_data_stream";

    sink.setDefaultStream(outputStreamName);
    sink.setDefaultPartition("0");

    return sink;
  }
}
```

Here we have added a Kinesis Data Stream of name `in-app-log-stream` as the source and another Kinesis Data Stream of name `log_data_stream` as the sink. Now we need to compile and package this code for deploying to the Kinesis Data Analytics service. We will see this in the next section.

### Deploying the Flink Application to Kinesis Data Analytics

Kinesis Data Analytics runs Flink applications by creating a job. It looks for a compiled source in an S3 bucket. Since our Flink application is in a Maven project, we will need to compile and package our application. using Maven as shown below:

```shell
mvn package -Dflink.version=1.13.2
```

Running this command will create a fat uber jar with all the dependencies which we will upload to an S3 bucket.

We will next create an application in Kinesis Data Analytics using the AWS administration console as shown below:

{{% image alt="Create Kinesis Data Analytics Application" src="images/posts/aws-kinesis/kda-app-create.png" %}}

An application is the Kinesis Data Analytics entity that we work with. We configure the three primary components in an application:

- **Input**: In the input configuration, we map the streaming source to an in-application data stream. Data flows from one or more data sources into the in-application data stream. We have configured a Kinesis Data Stream as a data source.
- **Application code**: Location of an S3 bucket containing the compiled Flink application that reads from an in-application data stream associated with a streaming source and writes to an in-application data stream associated with output.
- **Output**: one or more in-application streams to store intermediate results. We can then optionally configure an application output to persist data from specific in-application streams to an external destination.

Our application's dependent resources like CloudWatch Logs streams and IAM service roles also get created in this step.

After the application is created, we will configure the application with the location of the S3 bucket where we uploaded the uber jar of the Flink application that we created before.

### Running the Kinesis Data Analytics Application by Creating a Job

We can run our application by choosing `Run` on our application's page in the AWS console. When we run our Kinesis Data Analytics application, the Kinesis Data Analytics service creates an Apache Flink job.

The execution of the job, and the resources it uses, are managed by a Job Manager which separates the execution of the application into tasks. Each task is managed by a Task Manager. We examine the performance of each Task Manager, or the Job Manager as a whole to monitor the performance of our application.

### Creating Flink Application Interactively with Notebooks

The Flink application we built earlier was authored separately in a Java IDE (Eclipse) and then packaged and deployed in Kinesis Data Analytics by uploading the compiled artifact (jar file) to an S3 bucket. 

Instead of using an IDE like Eclipse, we can use notebooks which are more widely used for data science tasks, for authoring Flink applications. A notebook is a web-based interactive development environment where data scientists write and execute code and visualize results. 

Studio notebooks provided by Kinesis Data Streams use notebooks powered by [Apache Zeppelin](https://zeppelin.apache.org) and use Apache Flink as the stream processing engine.

We can create a Studio notebooks in the [AWS Management Console](https://ap-southeast-2.console.aws.amazon.com/kinesisanalytics/home?region=ap-southeast-2#/create-notebook) as shown below:

{{% image alt="Create Kinesis Data Analytics Notebooks" src="images/posts/aws-kinesis/kda_notebook.png" %}}

After we start the notebook, we can open it in Apache Zeppelin for writing code in SQL, Python, or Scala for developing applications using the notebook interface for Kinesis Data Streams, Amazon MSK, and S3 using built-in integrations, and various other streaming data sources with custom connectors. 

Using Studio notebooks, we model queries on streaming data using the [Apache Flink Table API and SQL](https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/table/overview/) in SQL, Python, Scala, or DataStream API in Scala. After that, we promote the Studio notebook to a continuously-running, non-interactive, Kinesis Data Analytics stream-processing application.

Please refer to the official [documentation](https://docs.aws.amazon.com/kinesisanalytics/latest/java/how-notebook.html) for details about using Studio notebook.

## Kinesis Video Streams - Ingesting, Storing, and Streaming Media

Kinesis Video Streams is a fully managed service that we can use to :
1. connect and stream video, audio, and other time-encoded data from various capturing devices using an infrastructure provisioned dynamically in the AWS Cloud
2. securely and durably store media data for a default retention period of `1` day and a maximum of `10` years.
3. build applications that operate on live data streams by consuming the ingested data frame-by-frame, in real-time for low-latency processing.
4. create batch or ad hoc applications that operate on durably persisted data without strict latency requirements. 

### Key Concepts: Producer, Consumer, and Kinesis Video Stream

The Kinesis Video Streams service is built around the concepts of a producer sending the streaming data to a stream and a consumer application reading that data from the stream.

* **Producer**: Any source that puts data into a Kinesis video stream. A producer can be any video-generating device, such as a security camera, a body-worn camera, a smartphone camera, or a dashboard camera. A producer can also send non-video data, such as audio feeds, images, or [RADAR](https://en.wikipedia.org/wiki/Radar) data.

* **Kinesis Video Stream**: A resource that transports live video data, optionally stores it, and makes the data available for consumption both in real-time and on a batch or ad hoc basis.

* **Consumer**: A consumer is an application that reads data like fragments and frames from a Kinesis Video Stream for viewing, processing, or analysis.

### Creating a Kinesis Video Stream

Let us first create a Kinesis Video Stream using the [AWS admin console](https://us-east-2.console.aws.amazon.com/kinesisvideo/home?region=us-east-2#/streams/create):
{{% image alt="Create Kinesis Data Analytics Application" src="images/posts/aws-kinesis/create_video_stream.png" %}} 
{{% image alt="Create Kinesis Data Analytics Application" src="images/posts/aws-kinesis/video_stream_created.png" %}}

Here we have created a new video stream with a default configuration. We can then use the Kinesis Video Streams API to put data into or read data from this video stream.

### Sending Media Data to a Kinesis Video Stream

Next, we need to configure a producer for putting data into this Kinesis Video Stream. The producer uses an application that extracts the video data in the form of frames from the media source and uploads it to the Kinesis Video Stream.

The producer uses a Kinesis Video Streams Producer SDK to extract the video data in the form of frames from the media source and sends it to the Kinesis Video Stream.

Kinesis Video Streams Producer SDK is used to build an on-device application that securely connects to a video stream, and reliably publishes video and other media data to Kinesis Video Stream.

It takes care of all the underlying tasks required to package the frames and fragments generated by the device's media pipeline. The SDK also handles stream creation, token rotation for secure and uninterrupted streaming, processing acknowledgments returned by Kinesis Video Streams, and other tasks.

Please refer to the [documentation](https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/producer-sdk.html) for details about using the Producer SDK.

### Consuming Media Data from a Kinesis Video Stream

We can consume media data by either viewing it in the AWS Kinesis Video Stream console or by creating an application that reads media data from a Kinesis Video Stream.

The Kinesis Video Stream Parser Library is a set of tools that can be used in Java applications to consume the [MKV](https://en.wikipedia.org/wiki/Matroska) data from a Kinesis Video Stream.

Please refer to the [documentation](https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/parser-library.html) for details about configuring the Parser library.

## Conclusion

Here is a list of the major points for a quick reference:

1. Streaming data is a pattern of data being generated continuously (in a stream) by multiple data sources which typically send the data records simultaneously. Due to its continuous nature, streaming data is also called unbounded data as opposed to bounded data handled by batch processing systems.
2. Amazon Kinesis is a family of managed services for collecting and processing streaming data in real-time.
3. Amazon Kinesis includes the following services each focussing on different stages of handling streaming data :
    * Kinesis Data Stream for ingestion and storage of streaming data
    * Kinesis Firehose for delivery of streaming data
    * Kinesis Analytics for running analysis programs over the ingested data for deriving analytical insights
    * Kinesis Video Streams for ingestion and storage of video data
4. The Kinesis Data Streams service is used to collect and process streaming data in real-time.
5. The Kinesis Data Stream is composed of multiple data carriers called shards. Each shard provides a fixed unit of capacity.
6. Kinesis Data Firehose is a fully managed service that is used to deliver streaming data to a destination.
7. Kinesis Data Analytics is used to analyze streaming data in real-time. It provides a fully managed service for running Apache Flink applications. Apache Flink is a Big Data processing framework for building applications that can process a large amount of data efficiently. Kinesis Data Analytics sets up the resources to run Flink applications and scales automatically to handle any volume of incoming data.
8. Kinesis Video Streams is a fully managed AWS service that we can use to ingest streaming video, audio, and other time-encoded data from various capturing devices using an infrastructure provisioned dynamically in the AWS Cloud.

You can refer to all the source code used in the article
on [Github](https://github.com/thombergs/code-examples/tree/master/aws/kinesis).

