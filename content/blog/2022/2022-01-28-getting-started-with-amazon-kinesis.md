---
authors: [pratikdas]
title: "Getting Started with AWS Kinesis"
categories: ["aws"]
date: 2022-01-20T00:00:00
excerpt: "Amazon Kinesis is a fully managed service for collecting and processing streaming data in real-time. Examples of streaming data are data collected from web site click-streams, marketing and financial information, social media feeds, iot sensors, and monitoring and operational logs. In this article, we will introduce Amazon Kinesis, understand its core concepts of the creating data streams, sending, and receiving data from streams and deriving analytical insights using different service variants: Kinesis Data Stream, firehose, Analytics, and Video Streams."
image: images/stock/0115-2021-1200x628-branded.jpg
url: getting-started-with-aws-kinesis
---

Amazon Kinesis is a fully managed service for collecting and processing streaming data in real-time. Examples of streaming data are data collected from web site click-streams, marketing and financial information, social media feeds, iot sensors, and monitoring and operational logs.

In this article, we will introduce Amazon Kinesis, understand its core concepts of the creating data streams, sending, and receiving data from streams and deriving analytical insights using different service variants: Kinesis Data Stream, firehose, Analytics, and Video Streams.

{{% github "https://github.com/thombergs/code-examples/tree/master/aws/kinesis" %}}

## What is Streaming Data

Streaming data is generated continuously (in a stream) by multiple data sources, which typically send in the data records simultaneously, and in small batches of size in the order of Kilobytes. 

Streaming data includes a wide variety of data such as log files generated by customers using mobile or web applications, ecommerce purchases, in-game player activity, information from social networks, financial trading floors, or geospatial services, and telemetry from connected devices or instrumentation in data centers.

Streaming data is processed sequentially and incrementally on a record-by-record basis or in batches aggregated over sliding time windows, and used for a wide variety of analytics including correlations, aggregations, filtering, and sampling.

## What is Amazon Kinesis
Amazon Kinesis is a fully managed streaming data platform for processing streaming data. It helps us to set up streams where producers can pump in data and receivers can consume data. Kinesis provides four specialized variants of services based on the type of stream processing we want to perform to suit our use case:

- **Kinesis Data Streams** : Amazon Kinesis Data Streams is a serverless streaming data service that 
we can use to build custom applications to process or analyze streaming data for specialized needs. We can add various types of data such as clickstreams, application logs, and social media to a Kinesis data stream from hundreds of thousands of sources.

- **Kinesis Data Firehose** : With Kinesis Data Firehose, we don't need to write applications or manage resources. We configure data producers to send data to Kinesis Data Firehose, and it automatically delivers the data to the specified destination. We can also configure Kinesis Data Firehose to transform the data before delivering it.

- **Kinesis Data Analytics**:With Amazon Kinesis Data Analytics for SQL Applications, you can process and analyze streaming data using standard SQL. The service enables you to quickly author and run powerful SQL code against streaming sources to perform time series analytics, feed real-time dashboards, and create real-time metrics.

- **Kinesis Video Streams**: Amazon Kinesis Video Streams is a fully managed AWS service that you can use to stream live video from devices to the AWS Cloud, or build applications for real-time video processing or batch-oriented video analytics.

Let us understand these services in the next sections. In each section is we will first introduce the key concepts of the service and then work through some examples.

## Kinesis Data Streams

Kinesis Data Streams is used to send data from data producers as soon as it is produced (in real-time) and then continuously processing that data. The processing can include transformation of the data before emitting to another data store or running real-time metrics and analytics.

When using Kinesis Data Streams, we first set up a data stream and then build producer applications which write data to the data stream and consumer applications that read data from the data stream:

// TODO diagram

As we can see in this diagram, the data stream is composed of multiple shards. The data is sent to a shard. Shards have an identifier called partition key which we use to identify the shard where we want to send our data. The data stored in the shard is called a record.

Each shard contains a sequence of data records. Each data record has a sequence number that is assigned by Kinesis Data Streams.


### Creating a Kinesis Data Stream
Let us first create our data stream where we can send our data. We can create a data stream either using the AWS Kinesis console or using the AWS SDK.

```java
public class DataStreamResourceHelper {

	public static void main(String[] args) {
		createDataStream();
	}
	

	public static void createDataStream() {
		 KinesisClient kinesisClient = getKinesisClient();
		 
		 CreateStreamRequest createStreamRequest 
		 = CreateStreamRequest
		 .builder()
		 .streamName(Constants.MY_DATA_STREAM)
		 .streamModeDetails(
		 	StreamModeDetails
		 	.builder()
		 	.streamMode(StreamMode.ON_DEMAND)
		 	.build())
		 .build();

		 CreateStreamResponse createStreamResponse 
		   = kinesisClient.createStream(createStreamRequest);
		 
		 DescribeStreamSummaryRequest describeStreamSummaryRequest 
						 = DescribeStreamSummaryRequest
						 .builder()
						 .streamName(Constants.MY_DATA_STREAM )
						 .build();

		 DescribeStreamSummaryResponse describeStreamSummaryResponse 
		    =  kinesisClient.describeStreamSummary(describeStreamSummaryRequest );
	
		 
		 long startTime = System.currentTimeMillis();
		 long endTime = startTime + ( 10 * 60 * 1000 );
		 while ( System.currentTimeMillis() < endTime ) {
			  try {
			    Thread.sleep(20 * 1000);
			  } 
			  catch ( Exception e ) {}
			  
			  try {
					 StreamDescriptionSummary streamDescSumm = describeStreamSummaryResponse.streamDescriptionSummary();
			  
					 if(streamDescSumm.streamStatus().equals(StreamStatus.ACTIVE)) break;
					  try {
					      Thread.sleep( 1000 );
					  }catch ( Exception e ) {}
			  }catch ( ResourceNotFoundException e ) {}
			  
			  
		 }
		 
	}
	
	private static KinesisClient getKinesisClient() {
		AwsCredentialsProvider credentialsProvider = 
		        ProfileCredentialsProvider.create(Constants.AWS_PROFILE_NAME);
		
		KinesisClient kinesisClient = KinesisClient
				.builder()
				.credentialsProvider(credentialsProvider)
				.region(Region.US_EAST_1).build();
		return kinesisClient;
	}

}

```

A Kinesis data stream is a set of shards. Each shard has a sequence of data records. Each data record has a sequence number that is assigned by Kinesis Data Streams.

### Data Ingestion - Writing data to Kinesis Data Streams

We can add data to a Kinesis data stream in different ways:
1. **AWS SDK**: with PutRecord and PutRecords operations
2. **Kinesis Producer Library (KPL)**:KPL is a library written in C++ for adding data into an Kinesis data stream. It runs as a child process to the main user process.
3. **Amazon Kinesis Agent**

Let us use the AWS Java SDK to add records to the Kinesis data stream created in the previous section. We need to first configure the kinesis library as a Maven dependency in our `pom.xml` as shown below:

```xml
   <dependencies>
  		<dependency>
		    <groupId>software.amazon.awssdk</groupId>
		    <artifactId>kinesis</artifactId>
		</dependency>
   </dependencies>
   <dependencyManagement>
    <dependencies>
      <dependency>
        <groupId>software.amazon.awssdk</groupId>
        <artifactId>bom</artifactId>
        <version>2.17.116</version>
        <type>pom</type>
        <scope>import</scope>
      </dependency>
    </dependencies>
   </dependencyManagement>

```

Here is the code for adding a single event to the Kinesis data stream:

```java
public class EventSender {

    private static final Logger logger = Logger
        .getLogger(EventSender.class.getName());

	public static void main(String[] args) {
		sendEvent();
	}
	
	public static void sendEvent() {
        KinesisClient kinesisClient = getKinesisClient();
        
		String partitionKey = String.format("partitionKey-%d", 1);
		String sequenceNumberForOrdering = "1";
		SdkBytes data 
		= SdkBytes.fromByteBuffer(
			ByteBuffer.wrap("Test data".getBytes()));

		PutRecordRequest putRecordRequest 
		   = PutRecordRequest
		   .builder()
		   .streamName(Constants.MY_DATA_STREAM)
		   .partitionKey(partitionKey)
		   .sequenceNumberForOrdering(sequenceNumberForOrdering)
		   .data(data)
		   .build();
		
		 PutRecordResponse putRecordsResult 
		 = kinesisClient.putRecord(putRecordRequest);
        
		 logger.info("Put Result" + putRecordsResult);
         kinesisClient.close();
	}

	private static KinesisClient getKinesisClient() {
		AwsCredentialsProvider credentialsProvider = 
		       ProfileCredentialsProvider
		       .create(Constants.AWS_PROFILE_NAME);
		
		KinesisClient kinesisClient = KinesisClient
				.builder()
				.credentialsProvider(credentialsProvider)
				.region(Region.US_EAST_1).build();
		return kinesisClient;
	}
}
```

```shell
INFO: Put ResultPutRecordResponse(ShardId=shardId-000000000001, SequenceNumber=49626569155656830268862440193769593466823195675894743058)
```

Let us next add multiple events to the Kinesis data stream. We will do this by using the `putRecords()` method as shown below:

```java
public class EventSender {

    private static final Logger logger = Logger.getLogger(EventSender.class.getName());

	/**
	 * @param args
	 */
	public static void main(String[] args) {
		sendEvents();

	}
	
	public static void sendEvents() {
        KinesisClient kinesisClient = getKinesisClient();
        
		String partitionKey = String.format("partitionKey-%d", 1);
        
 
        List <PutRecordsRequestEntry> putRecordsRequestEntryList  = new ArrayList<>(); 
        for (int i = 0; i < 5; i++) {
        	SdkBytes data = SdkBytes
        			.fromByteBuffer(ByteBuffer.wrap(("Test data "+i).getBytes()));
        	
            PutRecordsRequestEntry putRecordsRequestEntry  
                    = PutRecordsRequestEntry.builder()
                    
                    .data(data)
                    .partitionKey(partitionKey)
                    .build();
            
            putRecordsRequestEntryList.add(putRecordsRequestEntry); 
        }
        

        PutRecordsRequest putRecordsRequest 
						        = PutRecordsRequest
						        .builder()
						        .streamName(Constants.MY_DATA_STREAM)
						        .records(putRecordsRequestEntryList)
						        .build();
        
		PutRecordsResponse putRecordsResult = kinesisClient
				.putRecords(putRecordsRequest);
		
        logger.info("Put Result" + putRecordsResult);
        kinesisClient.close();
	}

	private static KinesisClient getKinesisClient() {
      ...
      ...
	}
}

```

```shell
...ResultPutRecordsResponse(FailedRecordCount=0, 
	Records=[
	PutRecordsResultEntry(SequenceNumber=49626569155656830268862440193770802392642928158972051474, ShardId=shardId-000000000001), 

	PutRecordsResultEntry(SequenceNumber=49626569155656830268862440193772011318462542788146757650, ShardId=shardId-000000000001), 

	PutRecordsResultEntry(SequenceNumber=49626569155656830268862440193773220244282157417321463826, ShardId=shardId-000000000001), 

	PutRecordsResultEntry(SequenceNumber=49626569155656830268862440193774429170101772046496170002, ShardId=shardId-000000000001), 

	PutRecordsResultEntry(SequenceNumber=49626569155656830268862440193775638095921386675670876178, ShardId=shardId-000000000001)])
```

### Data Consumption - Reading Data from Kinesis Data Streams

We need to build consumer applications for processing data from a Kinesis data stream. A consumer application can get its own 2 MB/sec allotment of read throughput when it uses enhanced fan-out. This will allow multiple consumers to read data from the same stream in parallel, without contending for read throughput with other consumers.

We can read data from a Kinesis data stream in different ways:
1. **AWS SDK**: with PutRecord and PutRecords operations
2. **Kinesis Consumer Library (KCL)**:KCL is a library written in C++ for adding data into an Kinesis data stream. It runs as a child process to the main user process.
3. **Amazon Kinesis Agent**

Let us use the AWS Java SDK to add records to the Kinesis data stream created in the previous section. We need to first configure the kinesis library as a Maven dependency in our `pom.xml` as shown below:

## Kinesis Data Firehose

Kinesis Firehose is a fully managed service which is built around the concept of a delivery stream. The delivery stream receives data from a data producer, optionally applies some transformation to the data before delivering the data to a destination.

The delivery stream buffers the incoming streaming data received from the data producer till it reaches a particular size or exceeds a certain time interval before delivering the data to the destination.

An example of a data producer is a web server that sends log data to a delivery stream. We can also configure the delivery stream to read data from a Kinesis data stream. 

A delivery stream can send data to the following destinations:

1. Amazon Simple Storage Service (Amazon S3), 
2. Amazon Redshift
3. Amazon OpenSearch Service
4. Splunk, and
5. Any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers like Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, and Sumo Logic.

### Creating a Delivery Stream

We can create a Kinesis Data Firehose delivery stream using the AWS Management Console or an AWS SDK. We need to provide a source of data along with a destination when creating a Firehose delivery stream.  

The source of a Kinesis Data Firehose delivery stream can be :
1. Kinesis Data Stream 
2. Producer 

### Sending Data to a Delivery Stream using Kinesis Data Streams


### Sending Data to a Delivery Stream using AWS SDK

### Sending Data to a Delivery Stream using Firehose Agent

### Data Transformation

Kinesis Data Firehose can invoke your Lambda function to transform incoming source data and deliver the transformed data to destinations. You can enable Kinesis Data Firehose data transformation when you create your delivery stream.

### Dynamic Partitioning


## Amazon Kinesis Data Analytics
Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real time. Using templates and built-in operators, you can quickly and easily build queries and stream real-time applications. Amazon Kinesis Data Analytics sets up the resources to run your applications and scales automatically to handle any volume of incoming data.

### Key Concepts
A Kinesis Data Analytics application has the following components:

Runtime properties: You can use runtime properties to configure your application without recompiling your application code.
Source: The application consumes data by using a source. A source connector reads data from a Kinesis data stream, an Amazon S3 bucket, etc. For more information, see Sources.
Operators: The application processes data by using one or more operators. An operator can transform, enrich, or aggregate data. For more information, see DataStream API Operators.
Sink: The application produces data to external sources by using sinks. A sink connector writes data to a Kinesis data stream, a Kinesis Data Firehose delivery stream, an Amazon S3 bucket, etc. For more information, see Sinks.

### Creating a Streaming Application

## Amazon Kinesis Video Streams
Amazon Kinesis Video Streams is a fully managed AWS service that you can use to stream live video from devices to the AWS Cloud, or build applications for real-time video processing or batch-oriented video analytics.

### Key Concepts

Producer – Any source that puts data into a Kinesis video stream. A producer can be any video-generating device, such as a security camera, a body-worn camera, a smartphone camera, or a dashboard camera. A producer can also send non-video data, such as audio feeds, images, or RADAR data.

Kinesis video stream – A resource that enables you to transport live video data, optionally store it, and make the data available for consumption both in real time and on a batch or ad hoc basis.

Consumer – Gets data, such as fragments and frames, from a Kinesis video stream to view, process, or analyze it. Generally these consumers are called Kinesis Video Streams applications.

### Creating a Kinesis Video Stream


### Sending Data to a Kinesis Video Stream


### Consuming Media Data
We can consume media data by either viewing it in the console, or by creating an application that reads media data from a stream using HLS.



Message Queueing is an asynchronous style of communication between two or more processes.

Messages and queues are the basic components of a message queuing system.

Programs communicate with each other by sending data in the form of messages which are placed in a storage called a queue, instead of calling each other directly. The receiver programs retrieve the message from the queue and do the processing without any knowledge of the producer programs.

This allows the communicating programs to run independently of each other, at different speeds and times, in different processes, and without having a direct connection between them.





## Conclusion

Here is a list of the major points for a quick reference:


You can refer to all the source code used in the article on [Github](https://github.com/thombergs/code-examples/tree/master/aws/sqs).

